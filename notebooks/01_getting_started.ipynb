{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Edge AI Agent - Getting Started\n",
        "\n",
        "A lightweight AI agent with tool-calling capabilities, powered by **TinyLlama** with 4-bit quantization.\n",
        "\n",
        "**Features:**\n",
        "- ðŸ§€ TinyLlama 1.1B with 4-bit quantization\n",
        "- ðŸ› ï¸ Tool calling: Calculator, Search, Weather\n",
        "- ðŸŽ¨ Gradio web interface\n",
        "\n",
        "> âš ï¸ Make sure to enable GPU accelerator (Settings > Accelerator > GPU)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers accelerate bitsandbytes gradio requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU:  {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LLM Module\n",
        "Load TinyLlama with 4-bit quantization for memory efficiency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "\n",
        "# 4-bit quantization config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")\n",
        "\n",
        "# Load model\n",
        "print('Loading model...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quant_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print('Model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_messages(messages):\n",
        "    formatted = ''\n",
        "    for msg in messages:\n",
        "        role, content = msg['role'], msg['content']\n",
        "        if role == 'system':\n",
        "            formatted += f'<|system|>\\\\n{content}</s>\\\\n'\n",
        "        elif role == 'user':\n",
        "            formatted += f'<|user|>\\\\n{content}</s>\\\\n'\n",
        "        elif role == 'assistant':\n",
        "            formatted += f'<|assistant|>\\\\n{content}</s>\\\\n'\n",
        "    formatted += '<|assistant|>\\\\n'\n",
        "    return formatted\n",
        "\n",
        "def generate(messages, max_new_tokens=256):\n",
        "    prompt = format_messages(messages)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Test Basic Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test basic chat\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': 'What is 2 + 2?'}\n",
        "]\n",
        "\n",
        "response = generate(messages)\n",
        "print(f'Response: {response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import requests\n",
        "\n",
        "def calculator(expr):\n",
        "    try:\n",
        "        allowed = {'abs': abs, 'min': min, 'max': max, 'pow': pow, 'pi': math.pi,\n",
        "                   'sqrt': math.sqrt, 'sin': math.sin, 'cos': math.cos}\n",
        "        return str(eval(expr, {'__builtins__': {}}, allowed))\n",
        "    except Exception as e:\n",
        "        return f'Error: {e}'\n",
        "\n",
        "def search_web(query):\n",
        "    try:\n",
        "        url = f'https://api.duckduckgo.com/?q={query}&format=json'\n",
        "        data = requests.get(url, timeout=10).json()\n",
        "        if data.get('Abstract'):\n",
        "            return data['Abstract']\n",
        "        elif data.get('RelatedTopics'):\n",
        "            results = [t.get('Text', '') for t in data['RelatedTopics'][:3] if t.get('Text')]\n",
        "            return '\\\\n'.join(results) if results else 'No results'\n",
        "        return 'No results'\n",
        "    except Exception as e:\n",
        "        return f'Error: {e}'\n",
        "\n",
        "def get_weather(city):\n",
        "    try:\n",
        "        resp = requests.get(f'https://wttr.in/{city}?format=3', timeout=10)\n",
        "        return resp.text.strip()\n",
        "    except Exception as e:\n",
        "        return f'Error: {e}'\n",
        "\n",
        "TOOLS = {\n",
        "    'calculator': {'desc': 'Math calculations', 'fn': calculator},\n",
        "    'search': {'desc': 'Web search', 'fn': search_web},\n",
        "    'weather': {'desc': 'Get weather', 'fn': get_weather}\n",
        "}\n",
        "\n",
        "print('Tools defined:', list(TOOLS.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Agent Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_system_prompt():\n",
        "    tools_desc = '\\\\n'.join([f'- {n}: {info[\"desc\"]}' for n, term == 'assistant' or role == 'user':

' for n, info in TOOLS.items()])\n",
        "    return f'''You are a helpful AI assistant with access to tools:\n",
        "\n",
        "{tools_desc}\n",
        "\n",
        "When you need a tool, respond with:\n",
        "<action>tool_name</action>\n",
        "<input>argument</input>\n",
        "\n",
        "Otherwise, respond directly.'''\n",
        "\n",
        "def parse_tool_call(response):\n",
        "    action = re.search(r'<action>(.*?)</action>', response, re.DOTALL)\n",
        "    input_m = re.search(r'<input>(.*?)</input>', response, re.DOTALL)\n",
        "    if action and input_m:\n",
        "        return action.group(1).strip(), input_m.group(1).strip()\n",
        "    return None, None\n",
        "\n",
        "def run_agent(user_input, max_iters=3):\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': get_system_prompt()},\n",
        "        {'role': 'user', 'content': user_input}\n",
        "    ]\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        response = generate(messages)\n",
        "        tool_name, tool_input = parse_tool_call(response)\n",
        "        \n",
        "        if tool_name and tool_name in TOOLS:\n",
        "            result = TOOLS[tool_name]['fn'](tool_input)\n",
        "            print(f'ðŸ›  Tool: {tool_name}({tool_input}) -> {result}')\n",
        "            messages.append({'role': 'assistant', 'content': response})\n",
        "            messages.append({'role': 'user', 'content': f'Tool result: {result}'})\n",
        "        else:\n",
        "            return response\n",
        "    \n",
        "    return 'Could not complete task'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test Agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test calculator\n",
        "print('Test 1: Calculator')\n",
        "print(run_agent('What is 123 * 456?'))\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test weather\n",
        "print('Test 2: Weather')\n",
        "print(run_agent('What is the weather in Tokyo?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Gradio Interface (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(m{sage, history):\n",
        "    return run_agent(message)\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title='ðŸ¤– Edge AI Agent',\n",
        "    description='A lightweight AI agent with tools: calculator, search, weather',\n",
        "    examples=[\n",
        "        'What is 123 * 456?',\n",
        "        'Weather in New York?',\n",
        "        'Search: what is machine learning'\n",
        "    ]\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}